{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CDC_Bert_Xlnet.ipynb","provenance":[],"collapsed_sections":["-YpbdxbYfx6Q"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6BqLg04UoIU6","colab_type":"text"},"source":["## Link with github project folder"]},{"cell_type":"code","metadata":{"id":"4mRxMnt4Q8Vz","colab_type":"code","outputId":"74fa50f8-1f7e-48dd-b493-a8d9070a619b","executionInfo":{"status":"ok","timestamp":1574212607590,"user_tz":300,"elapsed":2801,"user":{"displayName":"Jason Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCRNqBeZjhNQpy4RATemyLQ_uEPDPPZ8iycOQqQFw=s64","userId":"16214674102372443196"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Wed Nov 20 01:16:45 2019       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_7T6jfH2s1wl","colab_type":"code","outputId":"b38c6b92-2f6e-42d8-e2d8-bc0dc6bbc617","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!git clone https://github.com/acmilannesta/Bert-embedding\n","!pip install keras-bert\n","# !git clone https://github.com/acmilannesta/eda_nlp"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'Bert-embedding'...\n","remote: Enumerating objects: 147, done.\u001b[K\n","remote: Counting objects: 100% (147/147), done.\u001b[K\n","remote: Compressing objects: 100% (145/145), done.\u001b[K\n","remote: Total 147 (delta 68), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (147/147), 11.52 MiB | 5.76 MiB/s, done.\n","Resolving deltas: 100% (68/68), done.\n","Collecting keras-bert\n","  Downloading https://files.pythonhosted.org/packages/df/fe/bf46de1ef9d1395cd735d8df5402f5d837ef82cfd348a252ad8f32feeaef/keras-bert-0.80.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.17.4)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.2.5)\n","Collecting keras-transformer>=0.30.0\n","  Downloading https://files.pythonhosted.org/packages/0a/57/496b1eab888171b0801a0a44d3245a7874b8d1cc04c1fbfdbb5e3327fc7a/keras-transformer-0.31.0.tar.gz\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.8.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.3.2)\n","Collecting keras-pos-embd>=0.10.0\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.22.0\n","  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n","Collecting keras-layer-normalization>=0.12.0\n","  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.7.0\n","  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n","Collecting keras-self-attention==0.41.0\n","  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n","Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.80.0-cp36-none-any.whl size=37923 sha256=2162ac7fc53525fc8ebe9aaaa16d66d85931c584e35dd898b02e32ef169c4290\n","  Stored in directory: /root/.cache/pip/wheels/63/dc/87/3260cb91f3aa32c0f85c5375429a30c8fd988bbb48f5ee21b0\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.31.0-cp36-none-any.whl size=13385 sha256=36ccfccd09b2285cc92767d0587854b6cc040dd6f0e084d6827d8d88c142c952\n","  Stored in directory: /root/.cache/pip/wheels/a3/c5/9a/5a5130240be614a7a6fa786765d7692ae97f82601e2161bb56\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=297e0d5448618b2ac1aa17dbad5be2c3991af36ffa11de13daaceaf9438e13bd\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=2df9ffbf0e8b707766da4842e642c7b6e2bb15a0cbebd352ed09ae18a9a16b59\n","  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=e00a7a5c44c55f2a3450afe5af06510bdcd01f43544ce046dc549caeb8c48448\n","  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=7d8f2087f7536a399020970f12afe225e6374383f9bd38c87b1f7c50d88e935f\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=0632bfe25131b6bcd4883da86e630637ff2d1428ae71a7f260baf8d539e512a4\n","  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=17d1141e64c3907b261083b4d50a9d3c673b56dcd5c688fa7caa3dbe6e6f2790\n","  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n","Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n","Successfully installed keras-bert-0.80.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.31.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"306WTFXmGPA6","colab_type":"code","outputId":"387dfba3-cec4-4178-aa5d-d9ae5b9bc8d8","colab":{"base_uri":"https://localhost:8080/","height":81}},"source":["import json\n","import numpy as np\n","import pandas as pd\n","from random import choice\n","import re, os, gc\n","import codecs\n","import boto3\n","from keras.layers import *\n","from keras.losses import sparse_categorical_crossentropy\n","from keras.models import Model, load_model\n","import keras.backend as K\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import Callback, LearningRateScheduler\n","import tensorflow as tf\n","from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n","from sklearn.metrics import f1_score\n","from tqdm import tqdm\n","from functools import partial\n","from keras_bert import load_trained_model_from_checkpoint, Tokenizer, AdamWarmup, calc_train_steps, get_custom_objects, get_model\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","os.environ['AWS_SHARED_CREDENTIALS_FILE'] = 'AWS.txt'\n","s3 = boto3.Session(profile_name='default').client('s3')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Bhh2j65PSAgB","colab_type":"code","outputId":"8ec02e51-575c-499a-9131-da43fe99e587","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DKlLlzYSm9pR","colab_type":"text"},"source":["## Load Dataset\n","1. Add event weight\n","2. Reassign event code (0-47) for all\n","3. Add binary indicators for top 5 words by each event type\n"]},{"cell_type":"code","metadata":{"id":"mQK47tDfHL-U","colab_type":"code","colab":{}},"source":["train = pd.read_csv('Bert-embedding/CDC/train.csv')\n","test = pd.read_csv('Bert-embedding/CDC/test.csv')\n","# Event weight\n","wt = pd.DataFrame(train.event.value_counts()/len(train)).rename(columns={'event':'weight'})\n","wt['event'] = wt.index\n","train = train.merge(wt, how='left', on='event')\n","# Reassign eventcode\n","train['event_idx'] = train.event.map({y:x for x, y in enumerate(np.sort(train.event.unique()))})\n","# Assign weight freqency\n","train['wt_freq'] = np.where(train.weight<0.01, 1, np.where(train.weight<0.05, 2, 3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDY40RM-n3jq","colab_type":"text"},"source":["## EDA: Word frequencies by Event Type"]},{"cell_type":"code","metadata":{"id":"cdyXVucz8h0y","colab_type":"code","colab":{}},"source":["# Worclouds for top 20 events \n","from itertools import groupby \n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","train['text_grouped'] = train.groupby('event')['text'].transform(lambda x: ' '.join(x))\n","x = train[['event', 'text_grouped', 'weight']].drop_duplicates('event').sort_values('weight', ascending=False)\n","x.reset_index(drop=True, inplace=True)\n","\n","f, ax = plt.subplots(10, 2, figsize=(30,30))\n","\n","for seq in range(20):\n","    string = x.loc[seq, 'text_grouped'].split(' ')\n","    counts = [(len(list(c)),i) for i,c in groupby(sorted(string)) if len(i)>3 and i!='WORK'] \n","    counts_dict = {x[1]:x[0] for x in counts}\n","\n","    wordcloud = WordCloud(height=200, width=200, margin=0, collocations=False).generate_from_frequencies(counts_dict)\n","    \n","    ax[seq//2, seq%2].imshow(wordcloud, interpolation='bilinear')\n","    ax[seq//2, seq%2].set_title('event:'+str(x.loc[seq, 'event']) #+' weight:'+str(round(x.loc[seq, 'weight'], 2)),\n","                    ,fontsize=16, color='white')\n","    ax[seq//2, seq%2].set_axis_off()\n","    ax[seq//2, seq%2].margins(x=0, y=0)\n","    plt.tight_layout(w_pad=0.025)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DygvhLqzW9b6","colab_type":"text"},"source":["# BERT"]},{"cell_type":"markdown","metadata":{"id":"wO11GkeioDer","colab_type":"text"},"source":["## Download BERT checkpoint and dictionary"]},{"cell_type":"code","metadata":{"id":"fgw-2XW8Geus","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n","!unzip uncased_L-12_H-768_A-12.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"54DOZcMKE4sl","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n","!unzip wwm_uncased_L-24_H-1024_A-16.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"chVfrfc0e1ZP","colab_type":"code","outputId":"ac920a58-7ac7-4060-d79b-32a013f066fb","colab":{"base_uri":"https://localhost:8080/","height":293}},"source":["!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n","!unzip uncased_L-24_H-1024_A-16.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-11-19 19:42:55--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c03::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1247797031 (1.2G) [application/zip]\n","Saving to: ‘uncased_L-24_H-1024_A-16.zip’\n","\n","uncased_L-24_H-1024 100%[===================>]   1.16G  87.4MB/s    in 14s     \n","\n","2019-11-19 19:43:09 (87.0 MB/s) - ‘uncased_L-24_H-1024_A-16.zip’ saved [1247797031/1247797031]\n","\n","Archive:  uncased_L-24_H-1024_A-16.zip\n","   creating: uncased_L-24_H-1024_A-16/\n","  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n","  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  "],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RSngi92Ym2az","colab_type":"text"},"source":["## Parameter setting"]},{"cell_type":"code","metadata":{"id":"eJhMtLYVGl0N","colab_type":"code","colab":{}},"source":["MAXLEN = 142 #@param {type:'slider', min:50, max:300, step:1}\n","BATCH_SIZE = 16 #@param {type:'slider', min:8, max:32, step:8}\n","NUM_EPOCHS = 3\n","NUM_CLASSES = 48\n","LR = 4e-5\n","MIN_LR = 0\n","# OUTPUT_TRAIN = 'train_bert_ipredcv15_oof.csv'\n","# OUTPUT_TEST = 'test_base_cv15.npy'\n","choice = 'uncased_L-24_H-1024_A-16' #@param ['uncased_L-12_H-768_A-12', 'wwm_uncased_L-24_H-1024_A-16', 'uncased_L-24_H-1024_A-16']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCK8Uhcjmvmc","colab_type":"text"},"source":["## Tokenize train and validation set"]},{"cell_type":"code","metadata":{"id":"-HC_n547GuYj","colab_type":"code","colab":{}},"source":["token_dict = {}\n","with codecs.open(os.path.join(choice, 'vocab.txt'), 'r', 'utf8') as reader:\n","    for line in reader:\n","        token = line.strip()\n","        token_dict[token] = len(token_dict)\n","tokenizer = Tokenizer(token_dict)\n","\n","# token_dict1 = {}\n","# with codecs.open(dict_path1, 'r', 'utf8') as reader:\n","#     for line in reader:\n","#         token = line.strip()\n","#         token_dict1[token] = len(token_dict1)\n","# tokenizer1 = Tokenizer(token_dict1)\n","\n","def convert_data(data_df, branch='training'):\n","    data_df = data_df.reset_index(drop=True)\n","    global tokenizer\n","    indices, indices1 = [], []\n","    for i in tqdm(range(len(data_df))):\n","        ids, segments = tokenizer.encode(data_df.loc[i, 'text'])\n","        # ids1, segments1 = tokenizer1.encode(data_df.loc[i, 'text'])\n","        indices.append(ids)\n","        # indices1.append(ids1)\n","    aux = data_df[['age', 'sex']].apply(lambda x: (x - min(x)) / (max(x)-min(x)))\n","    if branch=='training':\n","        targets = data_df['event_idx'] \n","        return indices, np.array(targets), np.array(aux)\n","    else:\n","        return indices, np.array(aux)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gN2Yb16Jmqxk","colab_type":"text"},"source":["## Data Generator"]},{"cell_type":"code","metadata":{"id":"gl-tLqJGHXij","colab_type":"code","colab":{}},"source":["def seq_padding(X, padding=0):\n","    L = [len(x) for x in X]\n","    ML = max(L)\n","    return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n","\n","class data_generator:\n","    def __init__(self, data, batch_size=BATCH_SIZE, branch='train'):\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.branch = branch\n","        self.steps = len(self.data) // self.batch_size\n","        if len(self.data) % self.batch_size != 0:\n","            self.steps += 1\n","\n","    def __len__(self):\n","        return self.steps\n","\n","    def __iter__(self):\n","        while True:\n","            if self.branch == 'train':\n","                np.random.shuffle(self.data)\n","            for i in range(self.steps):\n","                d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n","                X1 = seq_padding([x[0] for x in d])           \n","                X2 = np.zeros_like(X1)\n","                # X3 = seq_padding([x[1] for x in d])           \n","                # X4 = np.zeros_like(X3)\n","                if self.branch == 'test':\n","                    aux = np.array([x[1] for x in d])\n","                    yield [X1, X2, aux]\n","                else:\n","                    Y = np.array([x[1] for x in d])\n","                    aux = np.array([x[2] for x in d])\n","                    yield [X1, X2, aux], Y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjR06fNGIIdC","colab_type":"text"},"source":["##Model Assemble"]},{"cell_type":"code","metadata":{"id":"4fnY5nGsHgHz","colab_type":"code","colab":{}},"source":["def model_build(len_train):\n","    global NUM_CLASSES\n","    global BATCH_SIZE\n","    global NUM_EPOCHS\n","    global MIN_LR\n","    global LR\n","    global MAXLEN\n","\n","\n","    bert_model = load_trained_model_from_checkpoint(\n","        os.path.join(choice, 'bert_config.json'),\n","        os.path.join(choice, 'bert_model.ckpt'),\n","        seq_len = MAXLEN,\n","        trainable=True\n","    )\n","\n","\n","    # clinic_model = load_trained_model_from_checkpoint(\n","    #     config_path1,\n","    #     checkpoint_path1,\n","    #     seq_len = MAXLEN,\n","    #     trainable=True\n","    # )\n","\n","    x1_in = Input(shape=(None,))\n","    x2_in = Input(shape=(None,))\n","    # x3_in = Input(shape=(None,))\n","    # x4_in = Input(shape=(None,))\n","    aux_in = Input(shape=(2, ))\n","\n","    inputs = bert_model([x1_in, x2_in])\n","    bert = Lambda(lambda x: x[:, 0])(inputs)\n","\n","\n","    dense = concatenate([bert, aux_in])\n","    outputs = Dense(NUM_CLASSES, activation='softmax')(dense)\n","    # outputs = Average()(outputs)\n","    model = Model([x1_in, x2_in, aux_in], outputs)\n","\n","    decay_steps, warmup_steps = calc_train_steps(\n","        len_train,\n","        batch_size=BATCH_SIZE,\n","        epochs=NUM_EPOCHS,\n","    )\n","\n","    model.compile(\n","        loss='sparse_categorical_crossentropy',\n","        optimizer=AdamWarmup(\n","            decay_steps=decay_steps,\n","            warmup_steps=warmup_steps,\n","            lr=LR,\n","            min_lr=MIN_LR,\n","            ),\n","        metrics=['sparse_categorical_accuracy']\n","    )\n","    del bert_model\n","    gc.collect()\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0fWMbNYa7hu","colab_type":"text"},"source":["## Batchwise evaluation callback"]},{"cell_type":"code","metadata":{"id":"zg3lanyguZHn","colab_type":"code","colab":{}},"source":["class IntervalPrediction(Callback):\n","\n","    def __init__(self, test_data, pred, pred1, nsplits, fold):\n","        super(Callback, self).__init__()\n","        self.test_data = test_data\n","        self.pred = pred\n","        self.nsplits = nsplits\n","        self.fold = fold\n","        self.pred1 = pred1\n","    def on_epoch_end(self, epoch, logs={}):\n","        # self.seen += logs.get('num_steps', 1)\n","        if epoch == 2:\n","            self.pred += self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1) * 0.3 / self.nsplits\n","            model_file = 'model-oof-'+str(self.fold)+'-'+str(epoch+1)+'.h5'\n","            self.model.save('model.h5')\n","            s3.upload_file('model.h5', 'acmilannesta', 'ipred/'+model_file)\n","        if epoch == 3:\n","            tmp = self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1) / self.nsplits\n","            self.pred += tmp * 0.7\n","            self.pred1 = tmp\n","            model_file = 'model-oof-'+str(self.fold)+'-'+str(epoch+1)+'.h5'\n","            self.model.save('model.h5')\n","            s3.upload_file('model.h5', 'acmilannesta', 'ipred/'+model_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"31l7HU8ZHsAe","colab_type":"code","outputId":"e89fb7b6-156a-4ddc-9b26-8b4883a48b83","colab":{"base_uri":"https://localhost:8080/","height":835}},"source":["test_indices, test_aux = convert_data(test, branch='test')\n","pred = np.zeros((len(test), NUM_CLASSES))\n","# pred = np.load(OUTPUT_TEST)\n","kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=0)\n","idx = [x for x in kf.split(train, train.wt_freq)]\n","\n","for i, (tr_idx, val_idx) in enumerate(idx[5:6], 6):\n","    print('\\nFold - {:}\\n'.format(i))\n","    tr, val = train.loc[tr_idx], train.loc[val_idx]\n","    tr_x, tr_y, tr_aux = convert_data(tr)\n","    val_x, val_y, val_aux = convert_data(val)\n","    model = model_build(len_train=len(tr_x))\n","    train_D = data_generator(list(zip(tr_x, tr_y, tr_aux)))\n","    valid_D = data_generator(list(zip(val_x, val_y, val_aux)), branch='valid')\n","    test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n","    # ipred = IntervalPrediction(test_data=test_D, pred=pred, nsplits=kf.get_n_splits(), fold=i, pred1=pred1)\n","    model.fit_generator(\n","        train_D.__iter__(),\n","        steps_per_epoch=len(train_D),\n","        epochs=NUM_EPOCHS,\n","        # callbacks = [ipred]\n","    )\n","    oof_pred = model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n","    # train_aug.loc[val_idx, 'oof_pred'] = np.argmax(oof_pred, 1)\n","    print('oof - {:} f1_score - {:.4f}'.format(i, f1_score(val_y, np.argmax(oof_pred, 1), average='weighted')))\n","\n","    # pred += model.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n","    # np.save(OUTPUT_TEST, pred)\n","    # s3.upload_file(Filename=OUTPUT_TEST, Bucket='acmilannesta', Key='base/'+OUTPUT_TEST)\n","\n","    model_file = 'model-oof-'+str(i)+'.h5'\n","    model.save('model.h5')\n","    s3.upload_file(Filename='model.h5', Bucket='acmilannesta', Key='large_uncased/'+model_file)\n","\n","    del model\n","    gc.collect()\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 75864/75864 [00:12<00:00, 5874.77it/s]\n","  0%|          | 572/123163 [00:00<00:21, 5719.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Fold - 6\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 123163/123163 [00:21<00:00, 5618.09it/s]\n","100%|██████████| 30793/30793 [00:05<00:00, 5841.40it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/3\n","7698/7698 [==============================] - 2823s 367ms/step - loss: 0.6764 - sparse_categorical_accuracy: 0.8011\n","Epoch 2/3\n","5580/7698 [====================>.........] - ETA: 12:57 - loss: 0.3626 - sparse_categorical_accuracy: 0.8821"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"65tc6F7LW8IE","colab_type":"code","colab":{}},"source":["test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n","pred_md = np.zeros((len(test), NUM_CLASSES))\n","for i in range(2, 9):\n","    print('Fold - {:} - Prediction'.format(i))\n","    load_file = 'base/model-oof-' + str(i) + '.h5'\n","    s3.download_file(Bucket='acmilannesta', Key=load_file, Filename='model.h5')\n","    loaded = load_model('model.h5', custom_objects=get_custom_objects())\n","    pred_md += loaded.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n","    del loaded\n","    !rm model.h5\n","    gc.collect()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fr7t8hplhlM","colab_type":"code","colab":{}},"source":["model = model_build(len(test_D))\n","model.load_weights(model.h5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKp8L-hvbQXu","colab_type":"code","outputId":"31ddbff1-499b-487f-d255-459c9ff73baa","colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["test['event'] = np.argmax(pred, 1)\n","test.event = test.event.map({x: y for x, y in enumerate(np.sort(train.event.unique()))})\n","test.to_csv('solution.csv', index=False)\n","test.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>event</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>54 Y O F PUNCTURE WOUND OF FIINGER RE ATTACHIN...</td>\n","      <td>2</td>\n","      <td>54</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>22 YOM CONTUSION TO LT LOWER LEG S P MVC HIT B...</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>20 YOM PT WORKS IN A QUARRY  WAS ATTEMPTING TO...</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>71</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>38 YOF WAS WALKING AT WORK TWISTED HER LT ANKL...</td>\n","      <td>2</td>\n","      <td>38</td>\n","      <td>73</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>44 YOM C O LOW BACK PAIN AFTER LIFTING A BOX A...</td>\n","      <td>1</td>\n","      <td>44</td>\n","      <td>71</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  sex  age  event\n","0  54 Y O F PUNCTURE WOUND OF FIINGER RE ATTACHIN...    2   54     55\n","1  22 YOM CONTUSION TO LT LOWER LEG S P MVC HIT B...    1   22     24\n","2  20 YOM PT WORKS IN A QUARRY  WAS ATTEMPTING TO...    1   20     71\n","3  38 YOF WAS WALKING AT WORK TWISTED HER LT ANKL...    2   38     73\n","4  44 YOM C O LOW BACK PAIN AFTER LIFTING A BOX A...    1   44     71"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"5otYpFI9WUaH","colab_type":"code","outputId":"c735a4e2-0c6d-47eb-9ea2-2597af6bec77","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# fit on whole training set and make preds on testing set\n","train_x, train_y, train_aux = convert_data(train)\n","model = model_build(len(train_x))\n","train_D = data_generator(list(zip(train_x, train_y, train_aux)))\n","model.fit_generator(    \n","    train_D.__iter__(),\n","    steps_per_epoch=len(train_D),\n","    epochs=NUM_EPOCHS\n","    )\n","\n","test_indices, test_aux = convert_data(test, branch='test')\n","test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n","pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n","test['bert_uncased'] = np.argmax(pred, 1)\n","test.to_csv('drive/My Drive/CDC Model/oof/test_bert_uncased_whole.csv', index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 75864/75864 [00:17<00:00, 4390.70it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2frFKaglPXeZ","colab_type":"code","colab":{}},"source":["model.save('drive/My Drive/CDC Model/M18.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TBwAPLDdSZg","colab_type":"code","colab":{}},"source":["# test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n","# pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n","test['event'] = np.argmax(pred, 1)\n","test['event'] = test.bert_clinic.map({x:y for x, y in enumerate(np.sort(train.event.unique()))})\n","test.drop('bert_clinic', 1).to_csv('solution.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jffzGhUxMgvV","colab_type":"text"},"source":["## Pseudo labeling"]},{"cell_type":"code","metadata":{"id":"6sAivZIJR54U","colab_type":"code","colab":{}},"source":["test = pd.read_csv('/content/drive/My Drive/CDC Model/solution_m14.csv')\n","test['event_idx'] = test.event.map({y:x for x, y in enumerate(np.sort(train.event.unique()))})\n","# test['event_idx'] = test.bert_uncased\n","test.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_66TD1bMgNf","colab_type":"code","outputId":"8006fe71-098d-454f-ce3c-aa0649024cd8","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tr, val = train_test_split(train, test_size=0.2, random_state=0)\n","tr_pseudo, val_pseudo = train_test_split(tr, test_size = 0.2, random_state = 1)\n","tr_pseudo = pd.concat([tr_pseudo, test, val])\n","# pseudo = pd.concat([train[['text', 'age', 'sex', 'event_idx']], test[['text', 'age', 'sex', 'event_idx']]], 0)\n","\n","tr_pseudo_x, tr_pseudo_y, tr_pseudo_aux = convert_data(tr_pseudo)\n","val_pseudo_x, val_pseudo_y, val_pseudo_aux = convert_data(val_pseudo)\n","pseudo_test_x, pseudo_test_aux = convert_data(test, branch='testing')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 75864/75864 [00:18<00:00, 4066.48it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kE4mRzhIOjBw","colab_type":"code","colab":{}},"source":["train_D = data_generator(list(zip(tr_pseudo_x, tr_pseudo_y, tr_pseudo_aux)))\n","test_D = data_generator(list(zip(pseudo_test_x, pseudo_test_aux)), branch='test')\n","valid_D = data_generator(list(zip(val_pseudo_x, val_pseudo_y, val_pseudo_aux)), branch='valid')\n","ival = IntervalEvaluation(validation_data=valid_D, label=val_pseudo_y, interval = len(train_D))\n","model = model_build(len(train_D))\n","model.fit_generator(    \n","    train_D.__iter__(),\n","    steps_per_epoch=len(train_D),\n","    epochs=2,\n","    callbacks = [ival]\n",")\n","pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n","test['event'] = np.argmax(pred, 1)\n","test['event'] = test.event.map({x:y for x, y in enumerate(np.sort(train.event.unique()))})\n","test[['text', 'sex', 'age', 'event']].to_csv('solution.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J5Kp5B8lXZHB","colab_type":"text"},"source":["# xlnet"]},{"cell_type":"code","metadata":{"id":"LFSP_E6DXcMd","colab_type":"code","outputId":"ddb62b4a-ee2b-4c1b-b4c1-87e9e19bcf82","colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["!wget https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip\n","!unzip cased_L-12_H-768_A-12.zip\n","# !wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n","# !unzip cased_L-24_H-1024_A-16.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-11-02 22:39:22--  https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c07::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 433638019 (414M) [application/zip]\n","Saving to: ‘cased_L-12_H-768_A-12.zip’\n","\n","cased_L-12_H-768_A- 100%[===================>] 413.55M  41.4MB/s    in 10s     \n","\n","2019-11-02 22:39:34 (40.3 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [433638019/433638019]\n","\n","Archive:  cased_L-12_H-768_A-12.zip\n","   creating: xlnet_cased_L-12_H-768_A-12/\n","  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt.index  \n","  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt.data-00000-of-00001  \n","  inflating: xlnet_cased_L-12_H-768_A-12/spiece.model  \n","  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt.meta  \n","  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_config.json  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2JyDeVwgXl1h","colab_type":"code","colab":{}},"source":["!pip install keras_xlnet\n","import os\n","from keras_xlnet import Tokenizer, load_trained_model_from_checkpoint, ATTENTION_TYPE_BI, ATTENTION_TYPE_UNI\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkJ0oUohX8pB","colab_type":"text"},"source":["## Parameter Setting"]},{"cell_type":"code","metadata":{"id":"F6WEeg-FX-P6","colab_type":"code","colab":{}},"source":["checkpoint_path = 'xlnet_cased_L-12_H-768_A-12' \n","MEMLEN=512\n","BATCH_SIZE=16"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AkPie-ovaz6l","colab_type":"text"},"source":["## Tokenize train and validation set"]},{"cell_type":"code","metadata":{"id":"gmC5x11c9Gg4","colab_type":"code","colab":{}},"source":["!pip install transformers\n","from transformers import XLNetTokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFCf5wiIa6O4","colab_type":"code","colab":{}},"source":["t = Tokenizer(os.path.join(checkpoint_path, 'spiece.model'))\n","def convert_data(data_df):\n","    # data_df.sample(frac=1, random_state=0)\n","    data_df.reset_index(drop=True, inplace=True)\n","    global tokenizer\n","    indices = []\n","    for i in tqdm(range(len(data_df))):\n","        ids = tokenizer.encode(data_df.loc[i, 'text'])\n","        indices.append(ids)\n","    targets = data_df['event_idx']\n","    aux = data_df[['age', 'sex']].apply(lambda x: (x - min(x)) / (max(x)-min(x)))\n","    return indices, np.array(targets), np.array(aux)\n","\n","tr, val = train_test_split(train, test_size=0.2, random_state=0)\n","tr_x, tr_y, tr_aux = convert_data(tr)\n","val_x, val_y, val_aux = convert_data(val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EZS6_5XzrUA","colab_type":"text"},"source":["## Data generator"]},{"cell_type":"code","metadata":{"id":"BlSZWoZJRWr6","colab_type":"code","outputId":"cbc7f92f-a269-441d-e678-67cc92d94dd9","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tokenizer.decode(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad>'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"S3bN05o_R7Ti","colab_type":"code","outputId":"43f402ad-f055-4aed-b05e-e133380e3dd2","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["t.SYM_PAD"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"v3JpOJehR3lS","colab_type":"code","colab":{}},"source":["tokenizer.encode(train.loc[0, 'text'], add_special_tokens=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mm8b_YTObQ1h","colab_type":"code","colab":{}},"source":["def seq_padding(X, padding=0):\n","  L = [len(x) for x in X]\n","  ML = max(L)\n","  return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n","\n","def seq_seg(X):\n","  seg = [[tokenizer.SYM_UNK]*(len(x)-1)+[tokenizer.SYM_EOS] for x in X]\n","  ML = max([len(x) for x in X])    \n","  return np.array([np.concatenate([x, [tokenizer.SYM_SEP] * (ML - len(x))]) if len(x) < ML else x for x in seg])\n","\n","def seq_mask(X):\n","  mask = [[tokenizer.SYM_UNK]*len(x) for x in X]\n","  ML = max([len(x) for x in X])    \n","  return np.array([np.concatenate([x, [tokenizer.SYM_BOS] * (ML - len(x))]) if len(x) < ML else x for x in mask])\n","\n","\n","class data_generator:\n","  def __init__(self, data, batch_size=BATCH_SIZE, memlen=MEMLEN, branch='train'):\n","    self.data = data\n","    self.batch_size = batch_size\n","    self.memlen = memlen\n","    self.branch = branch\n","    self.steps = len(self.data) // self.batch_size\n","    if len(self.data) % self.batch_size != 0:\n","        self.steps += 1\n","  def __len__(self):\n","    return self.steps\n","  def __iter__(self):\n","    while True:\n","        if self.branch=='train':\n","            np.random.shuffle(self.data)\n","        for i in range(self.steps):\n","            d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n","            X1 = seq_padding([x[0] for x in d])\n","            # segments\n","            X2 = seq_seg(([x[0] for x in d]))\n","            # memories\n","            X3 = np.array([self.memlen for i in range(len(d))])\n","            # masks\n","            # X4 = seq_mask(([x[0] for x in d]))\n","            Y = np.array([x[1] for x in d])\n","            aux = np.array([x[2] for x in d])\n","            yield [X1, X2, X3, aux], Y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YpbdxbYfx6Q","colab_type":"text"},"source":["## Model Assemble"]},{"cell_type":"code","metadata":{"id":"aLoDPW_fYQII","colab_type":"code","outputId":"695d8b27-d900-4db0-c2ca-e9b533312854","colab":{"base_uri":"https://localhost:8080/","height":424}},"source":["xlnet_model = load_trained_model_from_checkpoint(\n","    config_path=os.path.join(checkpoint_path, 'xlnet_config.json'),\n","    checkpoint_path=os.path.join(checkpoint_path, 'xlnet_model.ckpt'),\n","    batch_size=BATCH_SIZE, #16\n","    memory_len=MEMLEN, #512\n","    target_len=142, #128\n","    in_train_phase=False,\n","    attention_type=ATTENTION_TYPE_BI,\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xE9_d2diZjyL","colab_type":"code","outputId":"b2d65725-ad1e-40ca-d115-1bb5021dafad","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["x1_in = Input(shape=(None,))\n","x2_in = Input(shape=(None,))\n","x3_in = Input(shape=(1,))\n","# x4_in = Input(shape=(None,))\n","aux_in = Input(shape=(2, ))\n","\n","x = xlnet_model([x1_in, x2_in, x3_in])\n","x = Lambda(lambda x: x[:, 0])(x)\n","x = concatenate([x, aux_in])\n","p = Dense(48, activation='softmax')(x)\n","\n","model = Model([x1_in, x2_in, x3_in, aux_in], p)\n","\n","decay_steps, warmup_steps = calc_train_steps(\n","    len(tr_x),\n","    batch_size=BATCH_SIZE,\n","    epochs=2\n",")\n","\n","model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    # optimizer=Adam(1e-4),\n","    optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, learning_rate=1e-4, min_lr=1e-6),\n","    metrics= ['sparse_categorical_accuracy']\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zJJkN4CRkAaH","colab_type":"text"},"source":["## Batchwise evaluation callback"]},{"cell_type":"code","metadata":{"id":"4_cSeaqJkCk-","colab_type":"code","colab":{}},"source":["class IntervalEvaluation(Callback):\n","    def __init__(self, validation_data, label, weight, interval=3000):\n","        # super(Callback, self).__init__()\n","        self.seen = 0\n","        self.interval = interval\n","        self.validation_data = validation_data\n","        self.label = label\n","        self.weight = weight\n","    def on_batch_end(self, batch, logs={}):\n","        self.seen += logs.get('num_steps', 1)\n","        if self.seen % self.interval == 0:\n","            y_pred = self.model.predict_generator(self.validation_data.__iter__(), len(self.validation_data))\n","            score = f1_score(self.label, np.argmax(y_pred, 1), average='weighted', sample_weight=self.weight)\n","            print(\" - interval evaluation - batch: {:d} - score: {:.4f}\".format(self.seen, score))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"coVyQmw-jqDR","colab_type":"code","outputId":"5c9a6ff9-db3e-4229-b42a-93056dff75c1","colab":{"base_uri":"https://localhost:8080/"}},"source":["train_D = data_generator(list(zip(tr_x, tr_y, tr_aux)))\n","valid_D = data_generator(list(zip(val_x, val_y, val_aux)), branch='valid')\n","ival = IntervalEvaluation(validation_data=valid_D, label=val_y, weight=val_wt, interval = len(train_D))\n","model.fit_generator(\n","    train_D.__iter__(),\n","    steps_per_epoch=len(train_D),\n","    epochs=2,\n","    # validation_data=valid_D.__iter__(),\n","    # validation_steps=len(valid_D),\n","    callbacks = [ival]\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","Epoch 1/2\n","7697/7698 [============================>.] - ETA: 1s - loss: 0.7548 - sparse_categorical_accuracy: 0.7835 - interval evaluation - batch: 7698 - score: 0.9037\n","7698/7698 [==============================] - 12201s 2s/step - loss: 0.7548 - sparse_categorical_accuracy: 0.7835\n","Epoch 2/2\n","3641/7698 [=============>................] - ETA: 1:38:33 - loss: 0.4291 - sparse_categorical_accuracy: 0.8653WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","Epoch 1/2\n","7697/7698 [============================>.] - ETA: 1s - loss: 0.7548 - sparse_categorical_accuracy: 0.7835 - interval evaluation - batch: 7698 - score: 0.9037\n","7698/7698 [==============================] - 12201s 2s/step - loss: 0.7548 - sparse_categorical_accuracy: 0.7835\n","Epoch 2/2\n","7697/7698 [============================>.] - ETA: 1s - loss: 0.3980 - sparse_categorical_accuracy: 0.8731 - interval evaluation - batch: 15396 - score: 0.9122\n","7698/7698 [==============================] - 12191s 2s/step - loss: 0.3979 - sparse_categorical_accuracy: 0.8731\n"," - interval evaluation - batch: 15396 - score: 0.9122\n","7698/7698 [==============================] - 12191s 2s/step - loss: 0.3979 - sparse_categorical_accuracy: 0.8731\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fabe5c79c88>"]},"metadata":{"tags":[]},"execution_count":13},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fabe5c79c88>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"-LF-wzfvc9J-","colab_type":"code","outputId":"30c3d4c7-8151-49ce-8ba8-655e6ec86c56","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y_pred = model.predict_generator(valid_D.__iter__(), len(valid_D))\n","f1_score(val_y, np.argmax(y_pred, 1), average='weighted', sample_weight=val_wt)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8694244178850293"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"NLM_FwWD0BBd","colab_type":"code","colab":{}},"source":["del model\n","gc.collect()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0bZX1sxq-CW","colab_type":"code","colab":{}},"source":["model.save('drive/My Drive/CDC Model/xlnet_base.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FW8deZGmTK6u","colab_type":"text"},"source":["## Prediction on test set"]},{"cell_type":"code","metadata":{"id":"nHW8FC5uS_JM","colab_type":"code","outputId":"2627ea1f-7e2a-4faa-e707-35258ca291a0","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["test_indices = []\n","for i in tqdm(range(len(test))):\n","    ids = tokenizer.encode(test.loc[i, 'text'])\n","    ids.extend([tokenizer.SYM_SEP, tokenizer.SYM_CLS])\n","    test_indices.append(ids)\n","test_aux = np.array(test[['age', 'sex']].apply(lambda x: (x - min(x)) / (max(x)-min(x))))\n","\n","class test_generator:\n","  def __init__(self, data, batch_size=BATCH_SIZE, memlen=MEMLEN):\n","    self.data = data\n","    self.batch_size = batch_size\n","    self.memlen = memlen\n","    self.steps = len(self.data) // self.batch_size\n","    if len(self.data) % self.batch_size != 0:\n","        self.steps += 1\n","  def __len__(self):\n","    return self.steps\n","  def __iter__(self):\n","    while True:\n","        for i in range(self.steps):\n","            d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n","            X1 = seq_padding([x[0] for x in d])\n","            # segments\n","            X2 = seq_seg([x[0] for x in d])\n","            # memories\n","            X3 = np.array([self.memlen for i in range(len(d))])\n","            # masks\n","            # X4 = seq_mask(([x[0] for x in d]))\n","            # Y = np.array([x[1] for x in d])\n","            aux = np.array([x[1] for x in d])\n","            yield [X1, X2, X3, aux]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 75864/75864 [00:08<00:00, 8644.29it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_DnbdftZTHBB","colab_type":"code","outputId":"2b7168a9-2e16-4ee3-80ba-a18b62344f91","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["test_D = test_generator(list(zip(test_indices, test_aux)))\n","pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n","test['event'] = np.argmax(pred, 1)\n","test['event'] = test.event.map({x:y for x, y in enumerate(np.sort(train.event.unique()))})"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4742/4742 [==============================] - 2407s 508ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q6LYwEEMx70Z","colab_type":"code","colab":{}},"source":["test.to_csv('solution.csv', index=False)"],"execution_count":0,"outputs":[]}]}